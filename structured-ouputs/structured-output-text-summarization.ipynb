{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://cookbook.openai.com/examples/structured_outputs_intro\n",
    "# https://cookbook.openai.com/examples/structured_outputs_multi_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Summarization\n",
    "In this example, we will ask the model to summarize articles following a specific schema.\n",
    "\n",
    "This could be useful if you need to transform text or visual content into a structured object, for example to display it in a certain way or to populate database.\n",
    "\n",
    "We will take web scraping as an example, using Wikipedia articles discussing inventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preparation**. We will start by scraping content from multiple articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    html_content = soup.find(\"div\", class_=\"mw-parser-output\")\n",
    "    content = \"\\n\".join(p.text for p in html_content.find_all(\"p\"))\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    # Article on CNNs\n",
    "    \"https://en.wikipedia.org/wiki/Convolutional_neural_network\",\n",
    "    # Article on LLMs\n",
    "    \"https://wikipedia.org/wiki/Large_language_model\",\n",
    "    # Article on MoE\n",
    "    \"https://en.wikipedia.org/wiki/Mixture_of_experts\"\n",
    "]\n",
    "\n",
    "content = [get_article_content(url) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns features by itself via filter (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections.[1][2] For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,[3][4]  only 25 neurons are required to process 5x5-sized tiles.[5][6] Higher-layer features are extracted  from wider context windows, compared to lower-layer features.\\n\\nThey have applications in: \\n\\nCNNs are also known as shift invariant or space invariant artificial neural networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps.[12][13] Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.[14]\\n\\nFeed-forward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks makes them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.[15]\\n\\nConvolutional networks were inspired by biological processes[16][17][18][19] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\\n\\nCNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.[to whom?]\\n\\nA convolutional neural network consists of an input layer, hidden layers and an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer\\'s input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.\\nHere it should be noted how close a convolutional neural network is to a matched filter.[20]\\n\\nIn a CNN, the input is a tensor with shape:\\n\\n(number of inputs) × (input height) × (input width) × (input channels)\\n\\nAfter passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape:\\n\\n(number of inputs) × (feature map height) × (feature map width) × (feature map channels).\\n\\nConvolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus.[21] Each convolutional neuron processes data only for its receptive field. \\n\\nAlthough fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 × 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper.[5] For example, using a 5 × 5 tiling region, each with the same shared weights, requires only 25 neurons. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.[1][2]\\n\\nTo speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers,[22] which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of \\n\\n\\n\\n1\\n×\\n1\\n\\n\\n{\\\\displaystyle 1\\\\times 1}\\n\\n  kernels.\\n\\nConvolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 × 2 are commonly used. Global pooling acts on all the neurons of the feature map.[23][24] There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map,[25][26] while average pooling takes the average value.\\n\\nFully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.\\n\\nIn neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron\\'s receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.\\n\\nTo manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer. For example, atrous or dilated convolution[27][28] expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios,[29] thus having a variable receptive field size.\\n\\nEach neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights.\\n\\nThe vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.[30]\\n\\nCNN are often compared to the way the brain achieves vision processing in living organisms.[31]\\n\\nWork by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field.[32] Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space.[citation needed] The cortex in each hemisphere represents the contralateral visual field.[citation needed]\\n\\nTheir 1968 paper identified two basic visual cell types in the brain:[17]\\n\\nHubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.[33][32]\\n\\nThe \"neocognitron\"[16] was introduced by Kunihiko Fukushima in 1980.[18][26][34]\\nIt was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers:\\n\\nIn 1969, Fukushima had introduced the ReLU (rectified linear unit) activation function.[35][36]  It was not used in his neocognitron since all the weights were nonnegative; lateral inhibition was used instead.  The rectifier has become the most popular activation function for CNNs and  deep neural networks in general.[37]\\n\\nIn a variant of the neocognitron called the cresceptron, instead of using Fukushima\\'s spatial averaging with inhibition and saturation, J. Weng et al. in 1993 introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch.[38] Max-pooling is often used in modern CNNs.[39]\\n\\nSeveral supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron.[16] Today, however, the CNN architecture is usually trained through backpropagation.\\n\\nThe neocognitron is the first ANN which requires units located at multiple network positions to have shared weights, a hallmark of CNNs.\\n\\nThe term \"convolution\" first appears in neural networks in a paper by Toshiteru Homma, Les Atlas, and Robert Marks II at the first Conference on Neural Information Processing Systems in 1987. Their paper replaced multiplication with convolution in time, inherently providing shift invariance, motivated by and connecting more directly to the signal-processing concept of a filter, and demonstrated it on a speech recognition task.[6] They also pointed out that as a data-trainable system, convolution is essentially equivalent to correlation since reversal of the weights does not affect the final learned function (\"For convenience, we denote * as correlation instead of convolution. Note that convolving a(t) with b(t) is equivalent to correlating a(-t) with b(t).\").[6]  Modern CNN implementations typically do correlation and call it convolution, for convenience, as they did here.\\n\\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was one of the first convolutional networks, as it achieved shift-invariance.[40] A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation.[41] Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.[40]\\n\\nTDNNs are convolutional networks that share weights along the temporal dimension.[42] They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution.[43] Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts, as with images processed by a neocognitron.\\n\\nTDNNs improved the performance of far-distance speech recognition.[44]\\n\\nDenker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers.[45] However, the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.[46]\\n\\nFollowing the advances in the training of 1-D CNNs by Waibel et al. (1987), Yann LeCun et al. (1989)[46] used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types. \\nWei Zhang et al. (1988)[12][13] used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called shift-invariant pattern recognition neural network before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991)[47] and breast cancer detection in mammograms (1994).[48]\\n\\nThis approach became a foundation of modern computer vision.\\n\\nIn 1990 Yamaguchi et al. introduced the concept of max pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system.[25] In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.\\n\\nLeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1995,[49] classifies hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.\\n\\nIt was superior than other commercial courtesy amount reading systems (as of 1995). The system was integrated in NCR\\'s check reading systems, and fielded in several American banks since June 1996, reading millions of checks per day.[50]\\n\\nA shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.[12][13] It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.  The model was trained with back-propagation. The training algorithm was further improved in 1991[51] to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991)[47] and automatic detection of breast cancer in mammograms (1994).[48]\\n\\nA different convolution-based design was proposed in 1988[52] for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.[53][54]\\n\\nThe feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid[55] by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.\\n\\nAlthough CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).\\n\\nIn 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU.[56] In 2005, another paper also emphasised the value of GPGPU for machine learning.[57]\\n\\nThe first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU.[58] In the same period, GPUs were also used for unsupervised training of deep belief networks.[59][60][61][62]\\n\\nIn 2010, Dan Ciresan et al. at IDSIA trained deep feedforward networks on GPUs.[63] In 2011, they extended this to CNNs, accelerating by 60 compared to training CPU.[23] In 2011, the network win an image recognition contest where they achieved superhuman performance for the first time.[64] Then they won more competitions and achieved state of the art on several benchmarks.[65][39][26]\\n\\nSubsequently, AlexNet, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.[66] It was an early catalytic event for the AI boom.\\n\\nA very deep CNN with over 100 layers by Microsoft won the ImageNet 2015 contest.[67]\\n\\nCompared to the training of CNNs using GPUs, not much attention was given to the Intel Xeon Phi coprocessor.[68]\\nA notable development is a parallelization method for training convolutional neural networks on the Intel Xeon Phi, named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS).[69]\\nCHAOS exploits both the thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.\\n\\nIn the past, traditional multilayer perceptron (MLP) models were used for image recognition.[example  needed] However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher-resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights per fully-connected neuron, which is too high to feasibly process efficiently at scale.\\n\\nFor example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\\n\\nAlso, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.\\n\\nConvolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\\n\\nTogether, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.\\n\\n\\nA CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.\\nThe convolutional layer is the core building block of a CNN. The layer\\'s parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the filter entries and the input, producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.[72][nb 1]\\n\\nStacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input. Each entry in an activation map use the same set of parameters that define the filter.\\n\\nSelf-supervised learning has been adapted for use in convolutional layers by using sparse patches with a high-mask ratio and a global response normalization layer.[citation needed]\\n\\nWhen dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.\\n\\nThe extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learned (British English: learnt) filters produce the strongest response to a spatially local input pattern.\\n\\nThree hyperparameters control the size of the output volume of the convolutional layer: the depth, stride, and padding size:\\n\\nThe spatial size of the output volume is a function of the input volume size \\n\\n\\n\\nW\\n\\n\\n{\\\\displaystyle W}\\n\\n, the kernel field size \\n\\n\\n\\nK\\n\\n\\n{\\\\displaystyle K}\\n\\n of the convolutional layer neurons, the stride \\n\\n\\n\\nS\\n\\n\\n{\\\\displaystyle S}\\n\\n, and the amount of zero padding \\n\\n\\n\\nP\\n\\n\\n{\\\\displaystyle P}\\n\\n on the border. The number of neurons that \"fit\" in a given volume is then:\\n\\nIf this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be \\n\\n\\n\\nP\\n=\\n(\\nK\\n−\\n1\\n)\\n\\n/\\n\\n2\\n\\n\\n{\\\\textstyle P=(K-1)/2}\\n\\n when the stride is \\n\\n\\n\\nS\\n=\\n1\\n\\n\\n{\\\\displaystyle S=1}\\n\\n ensures that the input volume and output volume will have the same size spatially. However, it is not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.\\n\\nA parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a depth slice, the neurons in each depth slice are constrained to use the same weights and bias.\\n\\nSince all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron\\'s weights with the input volume.[nb 2] Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.[14]\\n\\nSometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a \"locally connected layer\".\\n\\nAnother important concept of CNNs is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling, where max pooling is the most common. It partitions the input image into a set of rectangles and, for each such sub-region, outputs the maximum.\\n\\nIntuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer) in a CNN architecture.[72]:\\u200a460–461\\u200a While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used.[14][71] The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2×2, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations:\\n\\n\\n\\n\\nf\\n\\nX\\n,\\nY\\n\\n\\n(\\nS\\n)\\n=\\n\\nmax\\n\\na\\n,\\nb\\n=\\n0\\n\\n\\n1\\n\\n\\n\\nS\\n\\n2\\nX\\n+\\na\\n,\\n2\\nY\\n+\\nb\\n\\n\\n.\\n\\n\\n{\\\\displaystyle f_{X,Y}(S)=\\\\max _{a,b=0}^{1}S_{2X+a,2Y+b}.}\\n\\n\\nIn this case, every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well).\\n\\nIn addition to max pooling, pooling units can use other functions, such as average pooling or ℓ2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice.[74]\\n\\nDue to the effects of fast spatial reduction of the size of the representation,[which?] there is a recent trend towards using smaller filters[75] or discarding pooling layers altogether.[76]\\n\\n\"Region of Interest\" pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.[citation needed]\\n\\nPooling is a downsampling method and an important component of convolutional neural networks for object detection based on the Fast R-CNN[77] architecture.\\n\\nA channel max pooling (CMP) operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels, which is important for fine-grained image classification that needs more discriminating features. Meanwhile, another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation, we denote the input feature maps and output feature maps of a CMP layer as F ∈ R(C×M×N) and C ∈ R(c×M×N), respectively, where C and c are the channel numbers of the input and output feature maps, M and N are the widths and the height of the feature maps, respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed, which is different from the MP operation.[78]\\n\\nReLU is the abbreviation of rectified linear unit introduced by Kunihiko Fukushima in 1969.[35][36] ReLU applies the non-saturating activation function \\n\\n\\n\\nf\\n(\\nx\\n)\\n=\\nmax\\n(\\n0\\n,\\nx\\n)\\n\\n\\n{\\\\textstyle f(x)=\\\\max(0,x)}\\n\\n.[66] It effectively removes negative values from an activation map by setting them to zero.[79] It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers.\\nIn 2011, Xavier Glorot, Antoine Bordes and Yoshua Bengio found that ReLU enables better training of deeper networks,[80] compared to widely used activation functions prior to 2011.\\n\\nOther functions can also be used to increase nonlinearity, for example the saturating hyperbolic tangent \\n\\n\\n\\nf\\n(\\nx\\n)\\n=\\ntanh\\n\\u2061\\n(\\nx\\n)\\n\\n\\n{\\\\displaystyle f(x)=\\\\tanh(x)}\\n\\n, \\n\\n\\n\\nf\\n(\\nx\\n)\\n=\\n\\n|\\n\\ntanh\\n\\u2061\\n(\\nx\\n)\\n\\n|\\n\\n\\n\\n{\\\\displaystyle f(x)=|\\\\tanh(x)|}\\n\\n, and the sigmoid function \\n\\n\\n\\nσ\\n(\\nx\\n)\\n=\\n(\\n1\\n+\\n\\ne\\n\\n−\\nx\\n\\n\\n\\n)\\n\\n−\\n1\\n\\n\\n\\n\\n{\\\\textstyle \\\\sigma (x)=(1+e^{-x})^{-1}}\\n\\n. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.[81]\\n\\nAfter several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).\\n\\nThe \"loss layer\", or \"loss function\", specifies how training penalizes the deviation between the predicted output of the network, and the true data labels (during supervised learning). Various loss functions can be used, depending on the specific task.\\n\\nThe Softmax loss function is used for predicting a single class of K mutually exclusive classes.[nb 3] Sigmoid cross-entropy loss is used for predicting K independent probability values in \\n\\n\\n\\n[\\n0\\n,\\n1\\n]\\n\\n\\n{\\\\displaystyle [0,1]}\\n\\n. Euclidean loss is used for regressing to real-valued labels \\n\\n\\n\\n(\\n−\\n∞\\n,\\n∞\\n)\\n\\n\\n{\\\\displaystyle (-\\\\infty ,\\\\infty )}\\n\\n.\\n\\nHyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP).\\n\\nThe kernel is the number of pixels processed together. It is typically expressed as the kernel\\'s dimensions, e.g., 2x2, or 3x3.\\n\\nPadding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad, that is 1 pixel on each side of the image.[citation needed]\\n\\nThe stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor.\\n\\nSince feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.\\n\\nThe number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.\\n\\nCommon filter sizes found in the literature vary greatly, and are usually chosen based on the data set. Typical filter sizes range from 1x1 to 7x7. As two famous examples, AlexNet used 3x3, 5x5, and 11x11. Inceptionv3 used 1x1, 3x3, and 5x5.\\n\\nThe challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and without overfitting.\\n\\nMax pooling is typically used, often with a 2x2 dimension. This implies that the input is drastically downsampled, reducing processing cost.\\n\\nGreater pooling reduces the dimension of the signal, and may result in unacceptable information loss. Often, non-overlapping pooling windows perform best.[74]\\n\\nDilation involves ignoring pixels within a kernel. This reduces processing/memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5, while still processing 9 (evenly spaced) pixels. Accordingly, dilation of 4 expands the kernel to 7x7.[citation needed]\\n\\nIt is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input.[71] However, layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal[71] While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happen in practice [82] and yield models that are not equivariant to translations.\\nFurthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input.[83][14] One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer.[71] Additionally, several other partial solutions have been proposed, such as anti-aliasing before downsampling operations,[84] spatial transformer networks,[85] data augmentation, subsampling combined with pooling,[14] and capsule neural networks.[86]\\n\\nThe accuracy of the final model is based on a sub-part of the dataset set apart at the start, often called a test-set. Other times methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction.[87][88]\\n\\nRegularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.\\n\\nBecause a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout, introduced in 2014.[89] At each training stage, individual nodes are either \"dropped out\" of the net (ignored) with probability \\n\\n\\n\\n1\\n−\\np\\n\\n\\n{\\\\displaystyle 1-p}\\n\\n or kept with probability \\n\\n\\n\\np\\n\\n\\n{\\\\displaystyle p}\\n\\n, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.\\n\\nIn the training stages, \\n\\n\\n\\np\\n\\n\\n{\\\\displaystyle p}\\n\\n is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored.\\n\\nAt testing time after training has finished, we would ideally like to find a sample average of all possible \\n\\n\\n\\n\\n2\\n\\nn\\n\\n\\n\\n\\n{\\\\displaystyle 2^{n}}\\n\\n dropped-out networks; unfortunately this is unfeasible for large values of \\n\\n\\n\\nn\\n\\n\\n{\\\\displaystyle n}\\n\\n. However, we can find an approximation by using the full network with each node\\'s output weighted by a factor of \\n\\n\\n\\np\\n\\n\\n{\\\\displaystyle p}\\n\\n, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates \\n\\n\\n\\n\\n2\\n\\nn\\n\\n\\n\\n\\n{\\\\displaystyle 2^{n}}\\n\\n neural nets, and as such allows for model combination, at test time only a single network needs to be tested.\\n\\nBy avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features[clarification needed] that better generalize to new data.\\n\\nDropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability \\n\\n\\n\\n1\\n−\\np\\n\\n\\n{\\\\displaystyle 1-p}\\n\\n. Each unit thus receives input from a random subset of units in the previous layer.[90]\\n\\nDropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.\\n\\nA major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.\\n\\nEven before Dropout, in 2013 a technique called stochastic pooling,[91] the conventional deterministic pooling operations were replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.\\n\\nAn alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images,[92] which delivers excellent performance on the MNIST data set.[92] Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.\\n\\nBecause the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train, especially considering that some part should be spared for later testing, two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s.[49] For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set.[93]\\n\\nOne of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.\\n\\nAnother simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a \"zero norm\".\\n\\nA simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant(\\'alpha\\' hyperparameter), thus increasing the penalty for large weight vectors.\\n\\nL2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.\\n\\nL1 regularization is also common. It makes the weight vectors sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization.\\n\\nAnother form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector \\n\\n\\n\\n\\n\\n\\nw\\n→\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\vec {w}}}\\n\\n of every neuron to satisfy \\n\\n\\n\\n‖\\n\\n\\n\\nw\\n→\\n\\n\\n\\n\\n‖\\n\\n2\\n\\n\\n<\\nc\\n\\n\\n{\\\\displaystyle \\\\|{\\\\vec {w}}\\\\|_{2}<c}\\n\\n. Typical values of \\n\\n\\n\\nc\\n\\n\\n{\\\\displaystyle c}\\n\\n are order of 3–4. Some papers report improvements[94] when using this form of regularization.\\n\\nPooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.[95]\\n\\nAn earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features\\' coordinate frame.[96]\\n\\nThus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.[97]\\n\\nCNNs are often used in image recognition systems. In 2012, an error rate of 0.23% on the MNIST database was reported.[26] Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database.[23] Subsequently, a similar CNN called AlexNet[98] won the ImageNet Large Scale Visual Recognition Challenge 2012.\\n\\nWhen applied to facial recognition, CNNs achieved a large decrease in error rate.[99] Another paper reported a 97.6% recognition rate on \"5,600 still images of more than 10 subjects\".[19] CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.[100]\\n\\nThe ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014,[101] a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet[102] (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans.[103] The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.[citation needed]\\n\\nIn 2015, a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.[104]\\n\\nCompared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space.[105][106] Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream.[107][108][109] Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies.[110][111] Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines[112] and Independent Subspace Analysis.[113] Its application can be seen in text-to-video model.[citation needed]\\n\\nCNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing,[114] search query retrieval,[115] sentence modeling,[116] classification,[117] prediction[118] and other traditional NLP tasks.[119]\\nCompared to traditional language processing methods such as recurrent neural networks, CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption, while RNNs are better suitable when classical time series modeling is required.[120][121][122][123]\\n\\nA CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.[124]\\n\\nCNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based drug design.[125] The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures,[126] AtomNet discovers chemical features, such as aromaticity, sp3 carbons, and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus[127] and multiple sclerosis.[128]\\n\\nCNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%.[129][130] It also earned a win against the program Chinook at its \"expert\" level of play.[131]\\n\\nCNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play.[132] Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.[133]\\n\\nA couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.[134]\\n\\nRecurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better.[135][11] Dilated convolutions[136] might enable one-dimensional convolutional neural networks to effectively learn time series dependences.[137] Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients.[138] Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from.[139] CNNs can also be applied to further tasks in time series analysis (e.g., time series classification[140] or quantile forecasting[141]).\\n\\nAs archaeological findings such as clay tablets with cuneiform writing are increasingly acquired using 3D scanners, benchmark datasets are becoming available, including HeiCuBeDa[142] providing almost 2000 normalized 2-D and 3-D datasets prepared with the GigaMesh Software Framework.[143] So curvature-based measures are used in conjunction with geometric neural networks (GNNs), e.g. for period classification of those clay tablets being among the oldest documents of human history.[144][145]\\n\\nFor many applications, training data is not very available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.[146]\\n\\nEnd-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars.[147] With recent advances in visual salience, spatial attention, and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.[148][149]\\n\\nA deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.[150]\\n\\nPreliminary results were presented in 2014, with an accompanying paper in February 2015.[151] The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.[152]\\n\\nConvolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR[153] have been obtained using CDBNs.[154]\\n', 'A large language model (LLM) is a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. Based on language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a computationally intensive self-supervised and semi-supervised training process.[1] LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.[2]\\n\\nLLMs are artificial neural networks that utilize the transformer architecture, invented in 2017. The largest and most capable LLMs, as of June\\xa02024[update], are built with a decoder-only transformer-based architecture, which enables efficient processing and generation of large-scale text data.\\n\\nHistorically, up to 2020, fine-tuning was the primary method used to adapt a model for specific tasks. However, larger models such as GPT-3 have demonstrated the ability to achieve similar results through prompt engineering, which involves crafting specific input prompts to guide the model\\'s responses.[3] These models acquire knowledge about syntax, semantics, and ontologies[4] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.[5]\\n\\nSome notable LLMs are OpenAI\\'s GPT series of models (e.g., GPT-3.5, GPT-4 and GPT-4o; used in ChatGPT and Microsoft Copilot), Google\\'s Gemini (the latter of which is currently used in the chatbot of the same name), Meta\\'s LLaMA family of models, Anthropic\\'s Claude models, and Mistral AI\\'s models.\\n\\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved then-SOTA perplexity.[6] In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"[7]), upon which they trained statistical language models.[8][9] In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.[10]\\n\\n\\nAfter neural networks became dominant in image processing around 2012, they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was before Transformers, it was done by seq2seq deep LSTM networks.\\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper\\'s goal was to improve upon 2014 Seq2seq technology,[11] and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014.[12] The following year in 2018, BERT was introduced and quickly became \"ubiquitous\".[13] Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model.\\n\\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use.[14] GPT-3 in 2020 went a step further and as of 2024[update] is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz.[15] The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities.[16] OpenAI did not reveal high-level architecture and the number of parameters of GPT-4.\\n\\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.[17]\\n\\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI\\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of June\\xa02024[update], The Instruction fine tuned variant of the Llama 3 70 billion parameter model is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.[18]\\n\\nAs of 2024, the largest and most capable models are all based on the Transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).[19][20][21]\\n\\nBecause machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indices are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding (BPE) and WordPiece. There are also special tokens serving as control characters, such as [MASK] for masked-out token (as used in BERT), and [UNK] (\"unknown\") for characters not appearing in the vocabulary.\\n\\nFor example, the BPE tokenizer used by GPT-3 (Legacy) would split tokenizer: texts -> series of numerical \"tokens\" as\\n\\nTokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.[22][23]\\n\\nAs an example, consider a tokenizer based on byte-pair encoding. In the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram, until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257).[24] After a tokenizer is trained, any text can be tokenized by it, as long as it does not contain characters not appearing in the initial-set of uni-grams.[25]\\n\\nA token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. An average word in another language encoded by such an English-optimized tokenizer is however split into suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for the Shan language from Myanmar. Even more widespread languages such as Portuguese and German have \"a premium of 50%\" compared to English.[26]\\n\\nGreedy tokenization also causes subtle problems with text completion.[27]\\n\\nIn the context of training LLMs, datasets are typically cleaned by removing toxic passages from the dataset, discarding low-quality data, and de-duplication.[28] Cleaned datasets can increase training efficiency and lead to improved downstream performance.[29][30] A trained LLM can be used to clean datasets for training a further LLM.[31]\\n\\nWith the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).[32]\\n\\nTraining of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. In these cases, synthetic data might be used. Microsoft\\'s Phi series of LLMs is trained on textbook-like data generated by another LLM.[33]\\n\\nReinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.[34]\\n\\nUsing \"self-instruct\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \"Write an essay about the main themes represented in Hamlet,\" an initial naive completion might be \"If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\" based on the frequency of this textual sequence in the corpus.[35]\\n\\nThe largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.[36][37][38]\\n\\nMost results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).[39]\\n\\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \"relevance\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model has had twelve attention heads and a context window of only 1k token.[41] In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.[25]\\n\\nThe largest models, such as Google\\'s Gemini 1.5, presented in February 2024, can have a context window sized up to 1 million (context window of 10 million was also \"successfully tested\").[42] Other models with large context windows includes Anthropic\\'s Claude 2.1, with a context window of up to 200k tokens.[43] Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example, the GPT-4 Turbo model has a maximum output of 4096 tokens.[44]\\n\\nLength of a conversation that the model can take into account when generating its next answer is limited by the size of a context window, as well. If the length of a conversation, for example with ChatGPT, is longer than its context window, only the parts inside the context window are taken into account when generating the next answer, or the model needs to apply some algorithm to summarize the too distant parts of conversation.\\n\\nThe shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context, while making it smaller can cause a model to miss an important long-range dependency. Balancing them are a matter of experimentation and domain-specific considerations.\\n\\nA model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset.[45] It can be either\\n\\nModels may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.[46] During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation.\\n\\nSubstantial infrastructure is necessary for training the largest models.[47][48][49]\\n\\nAdvances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million.[50][51][52] Since 2020, large sums were invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.[53]\\n\\nFor Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.[54]\\n\\nThere are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user\\'s input \\'354 * 139 = \\', provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response. Another example is \\'What is the time now? It is \\', where a separate program interpreter would need to execute a code to get system time on the computer, so LLM could include it in its reply.[55][56] This basic strategy can be sophisticated with multiple attempts of generated programs, and other sampling strategies.[57]\\n\\nGenerally, in order to get an LLM to use tools, one must finetune it for tool-use. If the number of tools is finite, then finetuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.[58][59]\\n\\nA simpler form of tool use is retrieval-augmented generation: the augmentation of an LLM with document retrieval. Given a query, a document retriever is called to retrieve the most relevant documents. This is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a vector database) most similar to the vector of the query. The LLM then generates an output based on both the query and context included from the retrieved documents.[60]\\n\\nAn LLM is a language model, which is not an agent as it has no goal, but it can be used as a component of an intelligent agent.[61] Researchers have described several methods for such integrations.[citation needed]\\n\\nThe ReAct pattern, a portmanteau of \"Reason\\xa0+\\xa0Act\", constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment.[62] The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.[63]\\n\\nIn the DEPS (\"Describe, Explain, Plan and Select\") method, an LLM is first connected to the visual world via image descriptions, then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives.[64]\\n\\nThe Reflexion method[65] constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up \"lessons learned\", which would help it perform better at a subsequent episode. These \"lessons learned\" are given to the agent in the subsequent episodes.[citation needed]\\n\\nMonte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.[66]\\n\\nFor open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent.[67] Alternatively, it can propose increasingly difficult tasks for curriculum learning.[68] Instead of outputting individual actions, an LLM planner can also construct \"skills\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.[68]\\n\\nLLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.[69]\\n\\nTypically, LLMs are trained with single- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters, requiring 200 gigabytes to load, which places them outside the range of most consumer electronics.[70]\\n\\nPost-training quantization[71] aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance.[72][73] The simplest form of quantization simply truncates all numbers to a given number of bits. It can be improved by using a different quantization codebook per layer. Further improvement can be done by applying different precisions to different parameters, with higher precision for particularly important parameters (\"outlier weights\").[74] See [75] for a visual guide.\\n\\nWhile quantized models are typically frozen, and only pre-quantized models are fine-tuned, quantized models can still be fine-tuned.[76]\\n\\nMultimodality means \"having several modalities\", and a \"modality\" refers to a type of input or output, such as video, image, audio, text, proprioception, etc.[77] There have been many AI models trained specifically to ingest one modality and output another modality, such as AlexNet for image to label,[78] visual question answering for image-text to text,[79] and speech recognition for speech to text.\\n\\nA common method to create multimodal models out of an LLM is to \"tokenize\" the output of a trained encoder. Concretely, one can construct an LLM that can understand images as follows: take a trained LLM, and take a trained image encoder \\n\\n\\n\\nE\\n\\n\\n{\\\\displaystyle E}\\n\\n. Make a small multilayered perceptron \\n\\n\\n\\nf\\n\\n\\n{\\\\displaystyle f}\\n\\n, so that for any image \\n\\n\\n\\ny\\n\\n\\n{\\\\displaystyle y}\\n\\n, the post-processed vector \\n\\n\\n\\nf\\n(\\nE\\n(\\ny\\n)\\n)\\n\\n\\n{\\\\displaystyle f(E(y))}\\n\\n has the same dimensions as an encoded token. That is an \"image token\". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.[80]\\n\\nFlamingo demonstrated the effectiveness of the tokenization method, finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch.[81] Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method, and applied to robotic control.[82] LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs,[83] and video inputs.[84]\\n\\nGPT-4 can use both text and image as inputs[85] (although the vision component was not released to the public until GPT-4V[86]); Google DeepMind\\'s Gemini is also multimodal.[87]\\n\\nThe following four hyper-parameters characterize an LLM:\\n\\nThey are related by simple statistical laws, called \"scaling laws\". One particular scaling law (\"Chinchilla scaling\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that:[88]\\n\\n\\n\\n\\n\\n\\n{\\n\\n\\n\\nC\\n=\\n\\nC\\n\\n0\\n\\n\\nN\\nD\\n\\n\\n\\n\\nL\\n=\\n\\n\\nA\\n\\nN\\n\\nα\\n\\n\\n\\n\\n+\\n\\n\\nB\\n\\nD\\n\\nβ\\n\\n\\n\\n\\n+\\n\\nL\\n\\n0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\begin{cases}C=C_{0}ND\\\\\\\\[6pt]L={\\\\frac {A}{N^{\\\\alpha }}}+{\\\\frac {B}{D^{\\\\beta }}}+L_{0}\\\\end{cases}}}\\n\\n where the variables are\\n\\nand the statistical hyper-parameters are\\n\\n\\nPerformance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by \"break(s)\"[89] in the scaling law, where the slope of the line changes abruptly, and where larger models acquire \"emergent abilities\".[39][90] They arise from the complex interaction of the model\\'s components and are not explicitly programmed or designed.[2]\\n\\nThe most intriguing among emergent abilities is in-context learning from example demonstrations.[91] In-context learning is involved in tasks, such as:\\n\\nSchaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.[97]\\n\\nLet \\n\\n\\n\\nx\\n\\n\\n{\\\\displaystyle x}\\n\\n be the number of parameter count, and \\n\\n\\n\\ny\\n\\n\\n{\\\\displaystyle y}\\n\\n be the performance of the model.\\n\\nLarge language models by themselves are \"black boxes\", and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work.\\n\\nMechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT, where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board, and modifying the representation changes the predicted legal Othello moves in the correct way.[98][99] In another example, a small Transformer is trained on Karel programs. Similar to the Othello-GPT example, there is a linear representation of Karel program semantics, and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.[100]\\n\\nIn another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.[101]\\n\\nNLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \"could (ever) understand natural language in some nontrivial sense\".[102] Proponents of \"LLM understanding\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to \"understand\" certain concepts. A Microsoft team argued in 2023 that GPT-4 \"can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\" and that GPT-4 \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system\": \"Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?\"[103][104] Some researchers characterize LLMs as \"alien intelligence\".[105][106] For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien \"Shoggoths\", and believes that RLHF tuning creates a \"smiling facade\" obscuring the inner workings of the LLM: \"If you don\\'t push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.\"[107][108]\\n\\nIn contrast, some proponents of the \"LLMs lack understanding\" school believe that existing LLMs are \"simply remixing and recombining existing writing\",[106] a phenomenon known as stochastic parrot, or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability.[102] For example, GPT-4 has natural deficits in planning and in real-time learning.[104] Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \"hallucination\".[109] Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input.[110] Neuroscientist Terrence Sejnowski has argued that \"The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\".[102]\\n\\nThe matter of LLM\\'s exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language.[102] These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL)[111] as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language.[112][113]\\n\\nThe most commonly used measure of a language model\\'s performance is its perplexity on a given text corpus. Perplexity is a measure of how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. Mathematically, perplexity is defined as the exponential of the average negative log likelihood per token:\\n\\n\\n\\nlog\\n\\u2061\\n(\\n\\nPerplexity\\n\\n)\\n=\\n−\\n\\n\\n1\\nN\\n\\n\\n\\n∑\\n\\ni\\n=\\n1\\n\\n\\nN\\n\\n\\nlog\\n\\u2061\\n(\\nPr\\n(\\n\\n\\ntoken\\n\\n\\ni\\n\\n\\n∣\\n\\n\\ncontext for token\\n\\n\\ni\\n\\n\\n)\\n)\\n\\n\\n{\\\\displaystyle \\\\log({\\\\text{Perplexity}})=-{\\\\frac {1}{N}}\\\\sum _{i=1}^{N}\\\\log(\\\\Pr({\\\\text{token}}_{i}\\\\mid {\\\\text{context for token}}_{i}))}\\n\\nhere \\n\\n\\n\\nN\\n\\n\\n{\\\\displaystyle N}\\n\\n is the number of tokens in the text corpus, and \"context for token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n\" depends on the specific type of LLM used. If the LLM is autoregressive, then \"context for token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n\" is the segment of text appearing before token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n. If the LLM is masked, then \"context for token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n\" is the segment of text surrounding token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n.\\n\\nBecause language models may overfit to their training data, models are usually evaluated by their perplexity on a test set of unseen data.[46] This presents particular challenges for the evaluation of large language models. As they are trained on increasingly large corpora of text largely scraped from the web, it becomes increasingly likely that models\\' training data inadvertently includes portions of any given test set.[3]\\n\\nIn information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon.[114] This relationship is mathematically expressed as \\n\\n\\n\\n\\nEntropy\\n\\n=\\n\\nlog\\n\\n2\\n\\n\\n\\u2061\\n(\\n\\nPerplexity\\n\\n)\\n\\n\\n{\\\\displaystyle {\\\\text{Entropy}}=\\\\log _{2}({\\\\text{Perplexity}})}\\n\\n.\\n\\nEntropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.\\n\\nNotably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different Large Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.\\n\\nIn the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model\\'s enhanced capability for compression. This, in turn, reflects the model\\'s proficiency in making accurate predictions.\\n\\nA large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, commonsense reasoning, and mathematical problem-solving.\\n\\nOne broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers, for example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\").[115] A question answering task is considered \"open book\" if the model\\'s prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"[115]). Otherwise, the task is considered \"closed book\", and the model must draw on knowledge retained during training.[116] Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.[116]\\n\\nEvaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".[3]\\n\\nSome composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, and HELM.[114][116] OpenAI has released tools for running composite benchmarks, but noted that the eval results are sensitive to the prompting method.[117][118] Some public datasets contain questions that are mislabeled, ambiguous, unanswerable, or otherwise of low-quality, which can be cleaned to give more reliable benchmark scores.[119]\\n\\nIt was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though researchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting).\\n\\nBecause of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly \"saturating\" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks.[120] In addition, there are cases of \"shortcut learning\" wherein AIs sometimes \"cheat\" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses, without necessarily understanding the actual question being asked.[102]\\n\\nSome datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer \"No\" to the question \"Can you teach an old dog new tricks?\" because of its exposure to the English idiom you can\\'t teach an old dog new tricks, even though this is not literally true.[121]\\n\\nAnother example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example:\\n\\nWe see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...\\na) demonstrates how to increase efficient exercise work by running up and down balls.\\nb) moves all his arms and legs and builds up a lot of muscle.\\nc) then plays the ball and we see a graphics and hedge trimming demonstration.\\nd) performs sit ups while on the ball and talking.[122]\\n\\nBERT selects b) as the most likely completion, though the correct answer is d).[122]\\n\\nIn 2023, Nature Biomedical Engineering wrote that \"it is no longer possible to accurately distinguish\" human-written text from text created by large language models, and that \"It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.\"[123] Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally.[124][125]\\n\\nMemorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates[126] or up to about 7%.[127]\\n\\nSome commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse.[128] For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.[129]\\n\\nA study by researchers at Google and several universities, including Cornell University and University of California, Berkeley, showed that there are potential security risks in language models such as ChatGPT. In their study, they examined and confirmed the possibility that questioners could get, from ChatGPT, the training data that the AI model used. For example, when asking ChatGPT 3.5 turbo to repeat the word \"poem\" forever, the AI model will say \"poem\" hundreds of times and then diverge, deviating from the standard dialogue style and spitting out nonsense phrases, thus spitting out the training data as it is. The researchers have seen more than 10,000 examples of the AI model exposing their training data in a similar method. The researchers said that it was hard to tell if the AI model was actually safe or not.[130]\\n\\nThe potential presence of \"sleeper agents\" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions.[131]\\n\\nLarge language model (LLM) applications accessible to the public, like ChatGPT or Claude, typically incorporate safety measures designed to filter out harmful content. However, implementing these controls effectively has proven challenging. For instance, research by Kang et al. [132]  demonstrated a method for circumventing LLM safety systems. Similarly, Wang\\n[133] illustrated how a potential criminal could potentially bypass ChatGPT 4o\\'s safety controls to obtain information on establishing a drug trafficking operation.\\n\\nWhile LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups.[134] Since English data is overrepresented in current large language models\\' training data, it may also downplay non-English views.[135]\\n\\nAI models can reinforce a wide range of stereotypes, including those based on gender, ethnicity, age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.[136]\\n\\nNotably, gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms.[134] For example, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.[137]\\n\\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[138]\\n\\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. Also, only the largest model\\'s cost is written.\\n', 'Mixture of experts (MoE) is a machine learning technique where multiple expert networks (learners) are used to divide a problem space into homogeneous regions.[1] It differs from ensemble techniques in that for MoE, typically only one or a few expert models are run for each input, whereas in ensemble techniques, all models are run on every input.\\n\\nIn mixture of experts, we always have the following ingredients, but they are constructed and combined differently.\\n\\nBoth the experts and the weighting function are trained by minimizing some form of loss function, generally by gradient descent. There is a lot of freedom in choosing the precise form of experts, the weighting function, and the loss function.\\n\\nThe meta-pi network, reported by Hampshire and Waibel,[2] uses \\n\\n\\n\\nf\\n(\\nx\\n)\\n=\\n\\n∑\\n\\ni\\n\\n\\nw\\n(\\nx\\n\\n)\\n\\ni\\n\\n\\n\\nf\\n\\ni\\n\\n\\n(\\nx\\n)\\n\\n\\n{\\\\displaystyle f(x)=\\\\sum _{i}w(x)_{i}f_{i}(x)}\\n\\n as the output. The model is trained by performing gradient descent on the mean-squared error loss \\n\\n\\n\\nL\\n:=\\n\\n\\n1\\nN\\n\\n\\n\\n∑\\n\\nk\\n\\n\\n‖\\n\\ny\\n\\nk\\n\\n\\n−\\nf\\n(\\n\\nx\\n\\nk\\n\\n\\n)\\n\\n‖\\n\\n2\\n\\n\\n\\n\\n{\\\\displaystyle L:={\\\\frac {1}{N}}\\\\sum _{k}\\\\|y_{k}-f(x_{k})\\\\|^{2}}\\n\\n. The experts may be arbitrary functions.\\n\\nIn their original publication, they were solving the problem of classifying phonemes in speech signal from 6 different Japanese speakers, 2 females and 4 males. They trained 6 experts, each being a \"time-delayed neural network\"[3] (essentially a multilayered convolution network over the mel spectrogram). They found that the resulting mixture of experts dedicated 5 experts for 5 of the speakers, but the 6th (male) speaker does not have a dedicated expert, instead his voice was classified by a linear combination of the experts for the other 3 male speakers.\\n\\nThe adaptive mixtures of local experts [4][5] uses a gaussian mixture model. Each expert simply predicts a gaussian distribution, and totally ignores the input. Specifically, the \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n-th expert predicts that the output is \\n\\n\\n\\ny\\n∼\\nN\\n(\\n\\nμ\\n\\ni\\n\\n\\n,\\nI\\n)\\n\\n\\n{\\\\displaystyle y\\\\sim N(\\\\mu _{i},I)}\\n\\n, where \\n\\n\\n\\n\\nμ\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle \\\\mu _{i}}\\n\\n is a learnable parameter. The weighting function is a linear-softmax function:\\n\\n\\n\\nw\\n(\\nx\\n\\n)\\n\\ni\\n\\n\\n=\\n\\n\\n\\ne\\n\\n\\nk\\n\\ni\\n\\n\\nT\\n\\n\\nx\\n+\\n\\nb\\n\\ni\\n\\n\\n\\n\\n\\n\\n∑\\n\\nj\\n\\n\\n\\ne\\n\\n\\nk\\n\\nj\\n\\n\\nT\\n\\n\\nx\\n+\\n\\nb\\n\\nj\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle w(x)_{i}={\\\\frac {e^{k_{i}^{T}x+b_{i}}}{\\\\sum _{j}e^{k_{j}^{T}x+b_{j}}}}}\\n\\nThe mixture of experts predict that the output is distributed according to the probability density function:\\n\\n\\n\\n\\nf\\n\\nθ\\n\\n\\n(\\ny\\n\\n|\\n\\nx\\n)\\n=\\nln\\n\\u2061\\n\\n[\\n\\n\\n∑\\n\\ni\\n\\n\\n\\n\\n\\ne\\n\\n\\nk\\n\\ni\\n\\n\\nT\\n\\n\\nx\\n+\\n\\nb\\n\\ni\\n\\n\\n\\n\\n\\n\\n∑\\n\\nj\\n\\n\\n\\ne\\n\\n\\nk\\n\\nj\\n\\n\\nT\\n\\n\\nx\\n+\\n\\nb\\n\\nj\\n\\n\\n\\n\\n\\n\\n\\nN\\n(\\ny\\n\\n|\\n\\n\\nμ\\n\\ni\\n\\n\\n,\\nI\\n)\\n\\n]\\n\\n=\\nln\\n\\u2061\\n\\n[\\n\\n(\\n2\\nπ\\n\\n)\\n\\n−\\nd\\n\\n/\\n\\n2\\n\\n\\n\\n∑\\n\\ni\\n\\n\\n\\n\\n\\ne\\n\\n\\nk\\n\\ni\\n\\n\\nT\\n\\n\\nx\\n+\\n\\nb\\n\\ni\\n\\n\\n\\n\\n\\n\\n∑\\n\\nj\\n\\n\\n\\ne\\n\\n\\nk\\n\\nj\\n\\n\\nT\\n\\n\\nx\\n+\\n\\nb\\n\\nj\\n\\n\\n\\n\\n\\n\\n\\n\\ne\\n\\n−\\n\\n\\n1\\n2\\n\\n\\n‖\\ny\\n−\\n\\nμ\\n\\ni\\n\\n\\n\\n‖\\n\\n2\\n\\n\\n\\n\\n\\n]\\n\\n\\n\\n{\\\\displaystyle f_{\\\\theta }(y|x)=\\\\ln \\\\left[\\\\sum _{i}{\\\\frac {e^{k_{i}^{T}x+b_{i}}}{\\\\sum _{j}e^{k_{j}^{T}x+b_{j}}}}N(y|\\\\mu _{i},I)\\\\right]=\\\\ln \\\\left[(2\\\\pi )^{-d/2}\\\\sum _{i}{\\\\frac {e^{k_{i}^{T}x+b_{i}}}{\\\\sum _{j}e^{k_{j}^{T}x+b_{j}}}}e^{-{\\\\frac {1}{2}}\\\\|y-\\\\mu _{i}\\\\|^{2}}\\\\right]}\\n\\nIt is trained by maximal likelihood estimation, that is, gradient ascent on \\n\\n\\n\\nf\\n(\\ny\\n\\n|\\n\\nx\\n)\\n\\n\\n{\\\\displaystyle f(y|x)}\\n\\n. The gradient for the \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n-th expert is\\n\\n\\n\\n\\n\\n\\n∇\\n\\n\\nμ\\n\\ni\\n\\n\\n\\n\\n\\nf\\n\\nθ\\n\\n\\n(\\ny\\n\\n|\\n\\nx\\n)\\n=\\n\\n\\n\\nw\\n(\\nx\\n\\n)\\n\\ni\\n\\n\\nN\\n(\\ny\\n\\n|\\n\\n\\nμ\\n\\ni\\n\\n\\n,\\nI\\n)\\n\\n\\n\\n∑\\n\\nj\\n\\n\\nw\\n(\\nx\\n\\n)\\n\\nj\\n\\n\\nN\\n(\\ny\\n\\n|\\n\\n\\nμ\\n\\nj\\n\\n\\n,\\nI\\n)\\n\\n\\n\\n\\n(\\ny\\n−\\n\\nμ\\n\\ni\\n\\n\\n)\\n\\n\\n{\\\\displaystyle \\\\nabla _{\\\\mu _{i}}f_{\\\\theta }(y|x)={\\\\frac {w(x)_{i}N(y|\\\\mu _{i},I)}{\\\\sum _{j}w(x)_{j}N(y|\\\\mu _{j},I)}}\\\\;(y-\\\\mu _{i})}\\n\\n\\n\\nand the gradient for the weighting function is\\n\\n\\n\\n\\n∇\\n\\n[\\n\\nk\\n\\ni\\n\\n\\n,\\n\\nb\\n\\ni\\n\\n\\n]\\n\\n\\n\\nf\\n\\nθ\\n\\n\\n(\\ny\\n\\n|\\n\\nx\\n)\\n=\\n\\n\\n[\\n\\n\\n\\nx\\n\\n\\n\\n\\n1\\n\\n\\n\\n]\\n\\n\\n\\n\\n\\nw\\n(\\nx\\n\\n)\\n\\ni\\n\\n\\n\\n\\n\\n∑\\n\\nj\\n\\n\\nw\\n(\\nx\\n\\n)\\n\\nj\\n\\n\\nN\\n(\\ny\\n\\n|\\n\\n\\nμ\\n\\nj\\n\\n\\n,\\nI\\n)\\n\\n\\n\\n(\\n\\nf\\n\\ni\\n\\n\\n(\\nx\\n)\\n−\\n\\nf\\n\\nθ\\n\\n\\n(\\ny\\n\\n|\\n\\nx\\n)\\n)\\n\\n\\n{\\\\displaystyle \\\\nabla _{[k_{i},b_{i}]}f_{\\\\theta }(y|x)={\\\\begin{bmatrix}x\\\\\\\\1\\\\end{bmatrix}}{\\\\frac {w(x)_{i}}{\\\\sum _{j}w(x)_{j}N(y|\\\\mu _{j},I)}}(f_{i}(x)-f_{\\\\theta }(y|x))}\\n\\n\\n\\nFor each input-output pair \\n\\n\\n\\n(\\nx\\n,\\ny\\n)\\n\\n\\n{\\\\displaystyle (x,y)}\\n\\n, the weighting function is changed to increase the weight on all experts that performed above average, and decrease the weight on all experts that performed below average. This encourages the weighting function to learn to select only the experts that make the right predictions for each input.\\n\\nThe \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n-th expert is changed to make its prediction closer to \\n\\n\\n\\ny\\n\\n\\n{\\\\displaystyle y}\\n\\n, but the amount of change is proportional to \\n\\n\\n\\nw\\n(\\nx\\n\\n)\\n\\ni\\n\\n\\nN\\n(\\ny\\n\\n|\\n\\n\\nμ\\n\\ni\\n\\n\\n,\\nI\\n)\\n\\n\\n{\\\\displaystyle w(x)_{i}N(y|\\\\mu _{i},I)}\\n\\n. This has a Bayesian interpretation. Given input \\n\\n\\n\\nx\\n\\n\\n{\\\\displaystyle x}\\n\\n, the prior probability that expert \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n is the right one is \\n\\n\\n\\nw\\n(\\nx\\n\\n)\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle w(x)_{i}}\\n\\n, and \\n\\n\\n\\nN\\n(\\ny\\n\\n|\\n\\n\\nμ\\n\\ni\\n\\n\\n,\\nI\\n)\\n\\n\\n{\\\\displaystyle N(y|\\\\mu _{i},I)}\\n\\n is the likelihood of evidence \\n\\n\\n\\ny\\n\\n\\n{\\\\displaystyle y}\\n\\n. So, \\n\\n\\n\\n\\n\\n\\nw\\n(\\nx\\n\\n)\\n\\ni\\n\\n\\nN\\n(\\ny\\n\\n|\\n\\n\\nμ\\n\\ni\\n\\n\\n,\\nI\\n)\\n\\n\\n\\n∑\\n\\nj\\n\\n\\nw\\n(\\nx\\n\\n)\\n\\nj\\n\\n\\nN\\n(\\ny\\n\\n|\\n\\n\\nμ\\n\\nj\\n\\n\\n,\\nI\\n)\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\frac {w(x)_{i}N(y|\\\\mu _{i},I)}{\\\\sum _{j}w(x)_{j}N(y|\\\\mu _{j},I)}}}\\n\\n is the posterior probability for expert \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n, and so the rate of change for the \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n-th expert is proportional to its posterior probability. \\n\\nIn words, the experts that, in hindsight, seemed like the good experts to consult, are asked to learn on the example. The experts that, in hindsight, were not, are left alone.\\n\\nThe combined effect is that the experts become specialized: Suppose two experts are both good at predicting a certain kind of input, but one is slightly better, then the weighting function would eventually learn to favor the better one. After that happens, the lesser expert is unable to obtain a high gradient signal, and becomes even worse at predicting such kind of input. Conversely, the lesser expert can become better at predicting other kinds of input, and increasingly pulled away into another region. This has a positive feedback effect, causing each expert to move apart from the rest and take care of a local region alone (thus the name \"local experts\").\\n\\nHierarchical mixtures of experts[6][7] uses multiple levels of gating in a tree. Each gating is a probability distribution over the next level of gatings, and the experts are on the leaf nodes of the tree. They are similar to decision trees.\\n\\nFor example, a 2-level hierarchical MoE would have a first order gating function \\n\\n\\n\\n\\nw\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle w_{i}}\\n\\n, and second order gating functions \\n\\n\\n\\n\\nw\\n\\nj\\n\\n|\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle w_{j|i}}\\n\\n and experts \\n\\n\\n\\n\\nf\\n\\nj\\n\\n|\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle f_{j|i}}\\n\\n. The total prediction is then \\n\\n\\n\\n\\n∑\\n\\ni\\n\\n\\n\\nw\\n\\ni\\n\\n\\n(\\nx\\n)\\n\\n∑\\n\\nj\\n\\n\\n\\nw\\n\\nj\\n\\n|\\n\\ni\\n\\n\\n(\\nx\\n)\\n\\nf\\n\\nj\\n\\n|\\n\\ni\\n\\n\\n(\\nx\\n)\\n\\n\\n{\\\\displaystyle \\\\sum _{i}w_{i}(x)\\\\sum _{j}w_{j|i}(x)f_{j|i}(x)}\\n\\n.\\n\\nThe mixture of experts, being similar to the gaussian mixture model, can also be trained by the expectation-maximization algorithm, just like gaussian mixture models. Specifically, during the expectation step, the \"burden\" for explaining each data point is assigned over the experts, and during the maximization step, the experts are trained to improve the explanations they got a high burden for, while the gate is trained to improve its burden assignment. This can converge faster than gradient ascent on the log-likelihood.[7][8]\\n\\nThe choice of gating function is often a softmax gating. Other than that, [9] proposed using gaussian distributions, and [8] proposed using exponential families.\\n\\nInstead of performing a weighted sum of all the experts, in hard MoE [10] only the highest ranked expert is chosen. That is, \\n\\n\\n\\nf\\n(\\nx\\n)\\n=\\n\\nf\\n\\narg\\n\\u2061\\n\\nmax\\n\\ni\\n\\n\\n\\nw\\n\\ni\\n\\n\\n(\\nx\\n)\\n\\n\\n(\\nx\\n)\\n\\n\\n{\\\\displaystyle f(x)=f_{\\\\arg \\\\max _{i}w_{i}(x)}(x)}\\n\\n. This can accelerate training and inference time.[11]\\n\\nThe experts can use more general forms of multivariant gaussian distributions. For example, [6] proposed \\n\\n\\n\\n\\nf\\n\\ni\\n\\n\\n(\\ny\\n\\n|\\n\\nx\\n)\\n=\\nN\\n(\\ny\\n\\n|\\n\\n\\nA\\n\\ni\\n\\n\\nx\\n+\\n\\nb\\n\\ni\\n\\n\\n,\\n\\nΣ\\n\\ni\\n\\n\\n)\\n\\n\\n{\\\\displaystyle f_{i}(y|x)=N(y|A_{i}x+b_{i},\\\\Sigma _{i})}\\n\\n, where \\n\\n\\n\\n\\nA\\n\\ni\\n\\n\\n,\\n\\nb\\n\\ni\\n\\n\\n,\\n\\nΣ\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle A_{i},b_{i},\\\\Sigma _{i}}\\n\\n are learnable parameters. In words, each expert learns to do linear regression, with a learnable uncertainty estimate.  \\n\\nOne can use different experts than gaussian distributions. For example, one can use Laplace distribution,[12] or Student\\'s t-distribution.[13] For binary classification, it also proposed logistic regression experts, with\\n\\n\\n\\n\\nf\\n\\ni\\n\\n\\n(\\ny\\n\\n|\\n\\nx\\n)\\n=\\n\\n\\n{\\n\\n\\n\\n\\n\\n1\\n\\n1\\n+\\n\\ne\\n\\n\\nβ\\n\\ni\\n\\n\\nT\\n\\n\\nx\\n+\\n\\nβ\\n\\ni\\n,\\n0\\n\\n\\n\\n\\n\\n\\n\\n,\\n\\n\\ny\\n=\\n0\\n\\n\\n\\n\\n1\\n−\\n\\n\\n1\\n\\n1\\n+\\n\\ne\\n\\n\\nβ\\n\\ni\\n\\n\\nT\\n\\n\\nx\\n+\\n\\nβ\\n\\ni\\n,\\n0\\n\\n\\n\\n\\n\\n\\n\\n,\\n\\n\\ny\\n=\\n1\\n\\n\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle f_{i}(y|x)={\\\\begin{cases}{\\\\frac {1}{1+e^{\\\\beta _{i}^{T}x+\\\\beta _{i,0}}}},&y=0\\\\\\\\1-{\\\\frac {1}{1+e^{\\\\beta _{i}^{T}x+\\\\beta _{i,0}}}},&y=1\\\\end{cases}}}\\n\\nwhere \\n\\n\\n\\n\\nβ\\n\\ni\\n\\n\\n,\\n\\nβ\\n\\ni\\n,\\n0\\n\\n\\n\\n\\n{\\\\displaystyle \\\\beta _{i},\\\\beta _{i,0}}\\n\\n are learnable parameters. This is later generalized for multi-class classification, with multinomial logistic regression experts.[14]\\n\\nThe previous section described MoE as it was used before the era of deep learning. After deep learning, MoE found applications in running the largest models, as a simple way to perform conditional computation: only parts of the model are used, the parts chosen according to what the input is.[15]\\n\\nThe earliest paper that applies MoE to deep learning is \"Learning Factored Representations in a Deep Mixture of Experts\" (Eigen, Ranzato, Sutskever) [16] which proposes to use a different gating network at each layer in a deep neural network. Specifically, each gating is a linear-ReLU-linear-softmax network, and each expert is a linear-ReLU network.\\n\\nThe key design desideratum for MoE in deep learning is to reduce computing cost. Consequently, for each query, only a small subset of the experts should be queried. This makes MoE in deep learning different from classical MoE. In classical MoE, the output for each query is a weighted sum of all experts\\' outputs. In deep learning MoE, the output for each query can only involve a few experts\\' outputs. Consequently, the key design choice in MoE becomes routing: given a batch of queries, how to route the queries to the best experts.\\n\\nThe sparsely-gated MoE layer,[17] published by researchers from Google Brain, uses feedforward networks as experts, and linear-softmax gating. Similar to the previously proposed hard MoE, they achieve sparsity by a weighted sum of only the top-k experts, instead of the weighted sum of all of them. Specifically, in a MoE layer, there are feedforward networks \\n\\n\\n\\n\\nf\\n\\n1\\n\\n\\n,\\n.\\n.\\n.\\n,\\n\\nf\\n\\nn\\n\\n\\n\\n\\n{\\\\displaystyle f_{1},...,f_{n}}\\n\\n, and a gating network \\n\\n\\n\\nw\\n\\n\\n{\\\\displaystyle w}\\n\\n. The gating network is defined by \\n\\n\\n\\nw\\n(\\nx\\n)\\n=\\n\\ns\\no\\nf\\nt\\nm\\na\\nx\\n\\n(\\n\\n\\nt\\no\\np\\n\\n\\nk\\n\\n\\n(\\nW\\nx\\n+\\n\\nnoise\\n\\n)\\n)\\n\\n\\n{\\\\displaystyle w(x)=\\\\mathrm {softmax} (\\\\mathrm {top} _{k}(Wx+{\\\\text{noise}}))}\\n\\n, where \\n\\n\\n\\n\\n\\nt\\no\\np\\n\\n\\nk\\n\\n\\n\\n\\n{\\\\displaystyle \\\\mathrm {top} _{k}}\\n\\n is a function that keeps the top-k entries of a vector the same, but sets all other entries to \\n\\n\\n\\n−\\n∞\\n\\n\\n{\\\\displaystyle -\\\\infty }\\n\\n. The addition of noise helps with load balancing. \\n\\nThe choice of \\n\\n\\n\\nk\\n\\n\\n{\\\\displaystyle k}\\n\\n is a hyperparameter that is chosen according to application. Typical values are \\n\\n\\n\\nk\\n=\\n1\\n,\\n2\\n\\n\\n{\\\\displaystyle k=1,2}\\n\\n. The \\n\\n\\n\\nk\\n=\\n1\\n\\n\\n{\\\\displaystyle k=1}\\n\\n version is also called the Switch Transformer.[18]\\n\\nAs demonstration, they trained a series of models for machine translation with alternating layers of MoE and LSTM, and compared with deep LSTM models.[19] Table 3 shows that the MoE models used less inference time compute, despite having 30x more parameters. \\n\\nVanilla MoE tend to have issues of load balancing: some experts are consulted often, while other experts rarely or not at all. To encourage the gate to select each expert with equal frequency (proper load balancing) within each batch, each MoE layer has two auxiliary loss functions. This is improved by [18] into a single auxiliary loss function. Specifically, let \\n\\n\\n\\nn\\n\\n\\n{\\\\displaystyle n}\\n\\n be the number of experts, then for a given batch of queries \\n\\n\\n\\n{\\n\\nx\\n\\n1\\n\\n\\n,\\n\\nx\\n\\n2\\n\\n\\n,\\n.\\n.\\n.\\n,\\n\\nx\\n\\nT\\n\\n\\n}\\n\\n\\n{\\\\displaystyle \\\\{x_{1},x_{2},...,x_{T}\\\\}}\\n\\n, the auxiliary loss for the batch is\\n\\n\\n\\nn\\n\\n∑\\n\\ni\\n=\\n1\\n\\n\\nn\\n\\n\\n\\nf\\n\\ni\\n\\n\\n\\nP\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle n\\\\sum _{i=1}^{n}f_{i}P_{i}}\\n\\nHere, \\n\\n\\n\\n\\nf\\n\\ni\\n\\n\\n=\\n\\n\\n1\\nT\\n\\n\\n#\\n(\\n\\nqueries sent to expert\\xa0\\n\\ni\\n)\\n\\n\\n{\\\\displaystyle f_{i}={\\\\frac {1}{T}}\\\\#({\\\\text{queries sent to expert }}i)}\\n\\n is the fraction of time where expert \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n is ranked highest, and \\n\\n\\n\\n\\nP\\n\\ni\\n\\n\\n=\\n\\n\\n1\\nT\\n\\n\\n\\n∑\\n\\nj\\n=\\n1\\n\\n\\nT\\n\\n\\n\\nw\\n\\ni\\n\\n\\n(\\n\\nx\\n\\nj\\n\\n\\n)\\n\\n\\n{\\\\displaystyle P_{i}={\\\\frac {1}{T}}\\\\sum _{j=1}^{T}w_{i}(x_{j})}\\n\\n is the fraction of weight on expert \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n. This loss is minimized at \\n\\n\\n\\n1\\n\\n\\n{\\\\displaystyle 1}\\n\\n, precisely when every expert has equal weight \\n\\n\\n\\n1\\n\\n/\\n\\nn\\n\\n\\n{\\\\displaystyle 1/n}\\n\\n in all situations.\\n\\nIn sparsely-gated MoE, only the top-k experts are queried, and their outputs are weighted-summed. There are other methods.[20]\\n\\nIn Hash MoE,[21] routing is performed deterministically by a hash function, fixed before learning begins. For example, if the model is a 4-layered Transformer, and input is a token for word \"eat\", and the hash of \"eat\" is \\n\\n\\n\\n(\\n1\\n,\\n4\\n,\\n2\\n,\\n3\\n)\\n\\n\\n{\\\\displaystyle (1,4,2,3)}\\n\\n, then the token would be routed to the 1st expert in layer 1, 4th expert in layer 2, etc. Despite its simplicity, it achieves competitive performance as sparsely gated MoE with \\n\\n\\n\\nk\\n=\\n1\\n\\n\\n{\\\\displaystyle k=1}\\n\\n. \\n\\nIn soft MoE, suppose in each batch, each expert can process \\n\\n\\n\\np\\n\\n\\n{\\\\displaystyle p}\\n\\n queries, then there are \\n\\n\\n\\nn\\n×\\np\\n\\n\\n{\\\\displaystyle n\\\\times p}\\n\\n queries that can be assigned per batch. Now for each batch of queries \\n\\n\\n\\n{\\n\\nx\\n\\n1\\n\\n\\n,\\n\\nx\\n\\n2\\n\\n\\n,\\n.\\n.\\n.\\n,\\n\\nx\\n\\nT\\n\\n\\n}\\n\\n\\n{\\\\displaystyle \\\\{x_{1},x_{2},...,x_{T}\\\\}}\\n\\n, the soft MoE layer computes an array \\n\\n\\n\\n\\nw\\n\\ni\\n,\\nj\\n,\\nk\\n\\n\\n\\n\\n{\\\\displaystyle w_{i,j,k}}\\n\\n, such that \\n\\n\\n\\n(\\n\\nw\\n\\ni\\n,\\nj\\n,\\n1\\n\\n\\n,\\n.\\n.\\n.\\n,\\n\\nw\\n\\ni\\n,\\nj\\n,\\nT\\n\\n\\n)\\n\\n\\n{\\\\displaystyle (w_{i,j,1},...,w_{i,j,T})}\\n\\n is a probability distribution over queries, and the \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n-th expert\\'s \\n\\n\\n\\nj\\n\\n\\n{\\\\displaystyle j}\\n\\n-th query is \\n\\n\\n\\n\\n∑\\n\\nk\\n\\n\\n\\nw\\n\\ni\\n,\\nj\\n,\\nk\\n\\n\\n\\nx\\n\\nk\\n\\n\\n\\n\\n{\\\\displaystyle \\\\sum _{k}w_{i,j,k}x_{k}}\\n\\n.[22] However, this does not work with autoregressive modelling, since the weights \\n\\n\\n\\n\\nw\\n\\ni\\n,\\nj\\n,\\nk\\n\\n\\n\\n\\n{\\\\displaystyle w_{i,j,k}}\\n\\n over one token depends on all other tokens\\'.[23]\\n\\nOther approaches include solving it as a constrained linear programming problem,[24] making each expert choose the top-k queries it wants (instead of each query choosing the top-k experts for it),[25] using reinforcement learning to train the routing algorithm (since picking an expert is a discrete action, like in RL).[26]\\n\\nSuppose there are \\n\\n\\n\\nn\\n\\n\\n{\\\\displaystyle n}\\n\\n experts in a layer. For a given batch of queries \\n\\n\\n\\n{\\n\\nx\\n\\n1\\n\\n\\n,\\n\\nx\\n\\n2\\n\\n\\n,\\n.\\n.\\n.\\n,\\n\\nx\\n\\nT\\n\\n\\n}\\n\\n\\n{\\\\displaystyle \\\\{x_{1},x_{2},...,x_{T}\\\\}}\\n\\n, each query is routed to one or more experts. For example, if each query is routed to one expert as in Switch Transformers, and if the experts are load-balanced, then each expert should expect on average \\n\\n\\n\\nT\\n\\n/\\n\\nn\\n\\n\\n{\\\\displaystyle T/n}\\n\\n queries in a batch. In practice, the experts cannot expect perfect load balancing: in some batches, one expert might be underworked, while in other batches, it would be overworked.\\n\\nSince the inputs cannot move through the layer until every expert in the layer has finished the queries it is assigned, load balancing is important. As a hard constraint on load balancing, there is the capacity factor: each expert is only allowed to process up to \\n\\n\\n\\nc\\n⋅\\nT\\n\\n/\\n\\nn\\n\\n\\n{\\\\displaystyle c\\\\cdot T/n}\\n\\n queries in a batch. [20] found \\n\\n\\n\\nc\\n∈\\n[\\n1.25\\n,\\n2\\n]\\n\\n\\n{\\\\displaystyle c\\\\in [1.25,2]}\\n\\n to work in practice.\\n\\nMoE layers are used in the largest transformer models, for which learning and inferring over the full model is too costly. They are typically sparsely-gated, with sparsity 1 or 2. In Transformer models, the MoE layers are often used to select the feedforward layers (typically a linear-ReLU-linear network), appearing in each Transformer block after the multiheaded attention. This is because the feedforward layers take up an increasing portion of the computing cost as models grow larger. For example, in the Palm-540B model, 90% of parameters are in its feedforward layers.[27]\\n\\nA trained Transformer can be converted to a MoE by duplicating its feedforward layers, with randomly initialized gating, then trained further. This is a technique called \"sparse upcycling\".[28]\\n\\nAs of 2023[update], models large enough to use MoE tend to be large language models, where each expert has on the order of 10 billion parameters. Other than language models, Vision MoE[29] is a Transformer model with MoE layers. They demonstrated it by training a model with 15 billion parameters. MoE Transformer has also been applied for diffusion models.[30]\\n\\nA series of large language models from Google used MoE. GShard[31] uses MoE with up to top-2 experts per layer. Specifically, the top-1 expert is always selected, and the top-2th expert is selected with probability proportional to that experts\\' weight according to the gating function. Later, GLaM[32] demonstrated a language model with 1.2 trillion parameters, each MoE layer using top-2 out of 64 experts. Switch Transformers[18] use top-1 in all MoE layers. \\n\\nThe NLLB-200 by Meta AI is a machine translation model for 200 languages.[33] Each MoE layer uses a hierarchical MoE with two levels. On the first level, the gating function chooses to use either a \"shared\" feedforward layer, or to use the experts. If using the experts, then another gating function computes the weights and chooses the top-2 experts.[34]\\n\\nMoE large language models can be adapted for downstream tasks by instruction tuning.[35]\\n\\nIn December 2023, Mistral AI released Mixtral 8x7B under Apache 2.0 license. It is a MoE language model with 46.7B parameters, 8 experts, and sparsity 2. They also released a version finetuned for instruction following.[36][37]\\n\\nIn March 2024, Databricks released DBRX. It is a MoE language model with 132B parameters, 16 experts, and sparsity 4. They also released a version finetuned for instruction following.[38][39]\\n']\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "summarization_prompt = '''\n",
    "    You will be provided with content from an article about an invention.\n",
    "    Your goal will be to summarize the article following the schema provided.\n",
    "    Here is a description of the parameters:\n",
    "    - invented_year: year in which the invention discussed in the article was invented\n",
    "    - summary: one sentence summary of what the invention is\n",
    "    - inventors: array of strings listing the inventor full names if present, otherwise just surname\n",
    "    - concepts: array of key concepts related to the invention, each concept containing a title and a description\n",
    "    - description: short description of the invention\n",
    "'''\n",
    "\n",
    "class ArticleSummary(BaseModel):\n",
    "    invented_year: int\n",
    "    summary: str\n",
    "    inventors: list[str]\n",
    "    description: str\n",
    "\n",
    "    class Concept(BaseModel):\n",
    "        title: str\n",
    "        description: str\n",
    "\n",
    "    concepts: list[Concept]\n",
    "\n",
    "def get_article_summary(text: str):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        temperature=0.2,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": summarization_prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        response_format=ArticleSummary,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing article #1...\n",
      "Done.\n",
      "Analyzing article #2...\n",
      "Done.\n",
      "Analyzing article #3...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "summaries = []\n",
    "\n",
    "for i in range(len(content)):\n",
    "    print(f\"Analyzing article #{i+1}...\")\n",
    "    summaries.append(get_article_summary(content[i]))\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(summary):\n",
    "    print(f\"Invented year: {summary.invented_year}\\n\")\n",
    "    print(f\"Summary: {summary.summary}\\n\")\n",
    "    print(\"Inventors:\")\n",
    "    for i in summary.inventors:\n",
    "        print(f\"- {i}\")\n",
    "    print(\"\\nConcepts:\")\n",
    "    for c in summary.concepts:\n",
    "        print(f\"- {c.title}: {c.description}\")\n",
    "    print(f\"\\nDescription: {summary.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTICLE 0\n",
      "\n",
      "Invented year: 1980\n",
      "\n",
      "Summary: A convolutional neural network (CNN) is a type of artificial neural network designed to process data with a grid-like topology, such as images, by using convolutional layers to automatically learn spatial hierarchies of features.\n",
      "\n",
      "Inventors:\n",
      "- Fukushima\n",
      "\n",
      "Concepts:\n",
      "- Convolutional Layers: These layers apply a convolution operation to the input, passing the result to the next layer. They are designed to automatically learn spatial hierarchies of features from input images.\n",
      "- Pooling Layers: Pooling layers reduce the dimensionality of each feature map but retain the most important information. This helps in reducing the computational load and controlling overfitting.\n",
      "- ReLU Activation Function: The rectified linear unit (ReLU) is a non-linear activation function used in CNNs to introduce non-linearity into the model, allowing it to learn complex patterns.\n",
      "- Weight Sharing: In CNNs, the same weights are used across different parts of the input, reducing the number of parameters and improving computational efficiency.\n",
      "- Applications: CNNs are widely used in image and video recognition, natural language processing, and other tasks that involve grid-like data structures.\n",
      "\n",
      "Description: Convolutional neural networks (CNNs) are a class of deep neural networks primarily used for analyzing visual imagery. They are designed to automatically and adaptively learn spatial hierarchies of features from input images, reducing the need for manual feature extraction. CNNs consist of an input layer, multiple hidden layers that perform convolutions, and an output layer, making them particularly effective for image and video recognition tasks.\n",
      "\n",
      "\n",
      "\n",
      "ARTICLE 1\n",
      "\n",
      "Invented year: 2017\n",
      "\n",
      "Summary: Large language models (LLMs) are advanced computational models that perform various natural language processing tasks using the transformer architecture.\n",
      "\n",
      "Inventors:\n",
      "- Vaswani\n",
      "- Shazeer\n",
      "- Parmar\n",
      "- Uszkoreit\n",
      "- Jones\n",
      "- Gomez\n",
      "- Kaiser\n",
      "- Polosukhin\n",
      "\n",
      "Concepts:\n",
      "- Transformer Architecture: Introduced in 2017, the transformer architecture is the foundation of LLMs, enabling efficient processing and generation of large-scale text data.\n",
      "- Generative AI: LLMs can generate text by predicting the next word or token in a sequence, a process known as text generation.\n",
      "- Prompt Engineering: A method used to guide LLMs in performing specific tasks by crafting precise input prompts.\n",
      "- Tokenization: The process of converting text into numerical tokens for processing by LLMs, often using methods like byte-pair encoding.\n",
      "- Reinforcement Learning from Human Feedback (RLHF): A technique to fine-tune LLMs based on human preferences to improve their performance.\n",
      "- Emergent Abilities: Unexpected capabilities that arise in LLMs as they scale, such as in-context learning from examples.\n",
      "- Bias and Ethics: LLMs can inherit biases from their training data, leading to skewed or unfair outputs.\n",
      "\n",
      "Description: Large language models (LLMs) are artificial neural networks utilizing the transformer architecture to perform tasks such as language generation and classification by learning from vast text data.\n",
      "\n",
      "\n",
      "\n",
      "ARTICLE 2\n",
      "\n",
      "Invented year: 1991\n",
      "\n",
      "Summary: Mixture of Experts (MoE) is a machine learning technique that uses multiple expert networks to handle different parts of a problem space, optimizing computational efficiency by activating only relevant experts for each input.\n",
      "\n",
      "Inventors:\n",
      "- Hampshire\n",
      "- Waibel\n",
      "\n",
      "Concepts:\n",
      "- Expert Networks: These are specialized models within the MoE framework that handle specific regions of the problem space, trained to perform well on particular types of inputs.\n",
      "- Gating Function: A mechanism that decides which expert networks to activate for a given input, typically using a softmax function to assign probabilities to each expert.\n",
      "- Gradient Descent: A common optimization technique used to train both the experts and the gating function by minimizing a loss function.\n",
      "- Hierarchical MoE: An extension of MoE that uses multiple levels of gating, similar to decision trees, to further refine the selection of experts.\n",
      "- Sparsely-Gated MoE: A variant of MoE where only a subset of experts are activated for each input, reducing computational cost and improving efficiency.\n",
      "- Load Balancing: A challenge in MoE where the gating function must ensure that all experts are utilized effectively, preventing some from being overused while others are underutilized.\n",
      "\n",
      "Description: Mixture of Experts (MoE) is a machine learning framework that divides a problem space into regions managed by specialized expert networks, with a gating mechanism determining which experts to activate for a given input, thereby optimizing computational resources.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(summaries)):\n",
    "    print(f\"ARTICLE {i}\\n\")\n",
    "    print_summary(summaries[i])\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
